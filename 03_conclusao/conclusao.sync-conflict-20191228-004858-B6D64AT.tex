\noindent\textbf{CONCLUSÃO}
$\!$\\

Ao longo deste trabalho, foi feita uma verificação dos conceitos teóricos da programação genética, e como ela pode ser utilizada para a resolução de problemas de controle. A programação genética busca otimizar programas de computador através de um processo inspirado na evolução biológica. As principais etapas desse processo envolvem a criação de soluções potenciais para o problema, seguido da avaliação de cada indivíduo, seleção dos candidatos mais aptos e aplicação subsequente dos operadores genéticos.

Vimos que as operações de cruzamento, mutação e replicação, possuem diferentes propósitos na busca por soluções. E a busca é guiada, em grande parte, pela função de aptidão; isto é, buscamos estabelecer um critério de avaliação que oriente o processo evolucionário de acordo com o objetivo. Muitas vezes, o critério mais prático e eficaz é a utilização de uma função custo, que calcula a diferença enter um estado de referência e uma situação atual influenciada pelo agente. 

Os algoritmos de aprendizagem por reforço utilizam uma função de recompensa, que fornece \textit{feedback} ao agente à cada instante de tempo. Muitas vezes essa função de recompensa é similar à uma função custo, logo, pode ser utilizada como critério aptidão para os indivíduos da programação genética. A partir deste conceito, este trabalho buscou realizar simulações e atribuir aptidões com base em uma função de recompensa. Dessa forma, foi possível aproveitar ambientes de simulações disponíveis em uma outra área de estudo, no caso, aprendizagem por reforço.

Python é uma das linguagens de programação mais populares em aplicações relacionadas à inteligência artificial. Com isso, diversas bibliotecas estão disponíveis, o que permite o rápido desenvolvimento de protótipos e testes de novas abordagens. A biblioteca Gym \cite{openaigym} fornece diversos ambientes de simulação, com uma interface relativamente simples, o que permite a comparação de algoritmos de aprendizagem de máquinas. Apesar de ser direcionada à algoritmos de aprendizagem por reforço, vimos que é possível utilizar algoritmos de aprendizagem evolucionária. A implementação dos agentes foi feita a partir da biblioteca DEAP, que permite a implementação de todas as etapas do ciclo evolucionário artificial, além de agrupar diversas ferramentas para a obtenção de estatísticas.

A integração dessas duas bibliotecas possibilitou ainda a comparação qualitativa da programação genética com algoritmos de aprendizagem por reforço.

Foi possível notar que o problema do pêndulo invertido foi solucionado de forma relativamente simples, já que na primeira geração alguns indivíduos obtiveram a aptidão máxima em 10 episódios. Entretanto, esta medida é uma aproximação do desempenho real do indivíduo, uma vez que algumas condições iniciais podem não ser resolvidas corretamente. Porém, o tempo de execução diminui consideravelmente com um baixo número de simulações para cada indivíduo.

Ao longo das gerações, a média de aptidão da população cresceu, conforme o esperado. Na décima quinta geração, a maior parte dos indivíduos possuía a aptidão máxima, indicando que o pêndulo permaneceu equilibrado durante 500 instantes de tempo. Na última geração, muitos indivíduos apresentaram o operador de soma e a variável terminal que indicava o ângulo do pêndulo. Isto indica que esses elementos foram eficazes para a resolução do problema abordado. A tendência de aumento do comprimento dos indivíduos ao longo das gerações também pôde ser observada, indicando a importância de exercer o controle sobre esse processo. Em termos de desempenho e tempo de execução, os resultados obtidos através da programação genética foram similares aos obtidos com o algoritmo DQN.

Para o segundo problema abordado, ficou evidente a possível complexidade das soluções que surgem a partir do processo evolucionário, com alguns indivíduos apresentando mais de 70 nós. Por exemplo, o indivíduo da Figura \ref{fig:4ec-pendulumindiv1} apresenta 41 nós, representando uma função matemática capaz de equilibrar um sistema de alta complexidade. Isso demonstra a capacidade da programação genética em aproximar funções utilizando apenas o conceito da evolução biológica. 

De certa forma, os indivíduos são sistemas que respondem à entradas, produzindo uma saída (ou sinal de controle), com isso, se assemelham muito à redes neurais, utilizadas de forma extensiva nos algoritmos DQN e DDPG. Umas das principais distinções entre as abordagens de aprendizagem por reforço e a programação genética reside na realização da otimização. Frequentemente, redes neurais são otimizadas a partir da propagação reversa do gradiente de uma função custo, em relação aos pesos da rede neural, enquanto métodos evolucionários são otimizações livres de gradiente (\textit{gradient free optimization}).

No ambiente do pêndulo duplo, o algoritmo DDPG apresentou-se superior. Entretanto, vale notar que não foram feitas mudanças significativas nos parâmetros da programação genética. É possível que algumas alterações, como por exemplo, nas probabilidade de ocorrência dos operadores genéticos, tenham um efeito positivo no desempenho do algoritmo. Os hiperparâmetros do algoritmo DDPG não foram alterados à partir da implementação utilizada \cite{stable-baselines}, com exceção do coeficiente de ruído das ações, um elemento que se mostrou importante para garantir o balanço entre exploração e convergência da rede neural.

Curiosamente, o melhor indivíduo da primeira execução da PG no pêndulo duplo possui uma estrutura bastante simples e, basicamente, verifica a velocidade do carrinho, utilizando-a para exercer um controle que varia entre os valores -1 e 1. Muitos dos problemas que podem ser resolvidos por buscas exaustivas no espaço de soluções costumam ser facilmente solucionados pela busca direcionada provida pela PG. 

O problema do carro na ladeira evidenciou a dificuldade do algoritmo de aprendizagem por reforço, quando a função de recompensa não é muito informativa. Basicamente, o sinal de recompensa é estável, até que o objetivo final seja alcançado. Dessa forma, o aprendizado é demorado e a programação genética se mostrou superior em tempo de execução e desempenho.




