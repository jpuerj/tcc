\noindent\textbf{CONCLUSÃO}
$\!$\\

Ao longo deste trabalho, foi feita uma verificação dos conceitos teóricos da programação genética, e como ela pode ser utilizada para a resolução de problemas de controle. A programação genética busca otimizar programas de computador através de um processo inspirado na evolução biológica. As principais etapas desse processo envolvem a criação de soluções potenciais para o problema, seguido da avaliação de cada indivíduo, seleção dos candidatos mais aptos e aplicação subsequente dos operadores genéticos.

As operações de cruzamento, mutação e replicação, possuem diferentes propósitos na busca por soluções. Esta busca é guiada, em grande parte, pela função de aptidão; isto é, busca-se estabelecer um critério de avaliação que oriente o processo evolucionário de acordo com o objetivo. Muitas vezes, o critério mais prático e eficaz é a utilização de uma função custo, que calcula a diferença entre um estado de referência e a situação atual, influenciada, em parte, pelo agente.

Os algoritmos de aprendizagem por reforço utilizam uma função de recompensa, que fornece um retorno ao agente, à cada instante de tempo. Muitas vezes essa função de recompensa é similar a uma função custo, logo, pode ser utilizada como critério de aptidão, para os indivíduos da programação genética. A partir deste conceito, este trabalho buscou realizar simulações e atribuir aptidões com base em uma função de recompensa. Dessa forma, foi possível aproveitar ambientes de simulações disponíveis em outra área de estudo, a aprendizagem por reforço.

Python é uma das linguagens de programação mais populares em aplicações relacionadas à inteligência artificial. Com isso, diversas bibliotecas estão disponíveis, o que permite o rápido desenvolvimento de protótipos e testes de novas abordagens. A biblioteca Gym \cite{openaigym} fornece diversos ambientes de simulação, com uma ``interface'' simples, o que permite a comparação eficaz de algoritmos de aprendizagem de máquinas. Apesar de ser direcionada a algoritmos de aprendizagem por reforço, é possível utilizar a abordagem evolucionária. A implementação dos agentes foi feita a partir da biblioteca DEAP, que permite criar todas as etapas do ciclo evolutivo artificial, além de agrupar diversas ferramentas para a obtenção de estatísticas.

A integração dessas duas bibliotecas possibilitou ainda a comparação qualitativa da programação genética com algoritmos de aprendizagem por reforço.

Foi possível notar que o problema do pêndulo invertido foi solucionado de forma relativamente simples, já que, na primeira geração, alguns indivíduos obtiveram a aptidão máxima em 10 episódios. Entretanto, esta medida é uma aproximação do desempenho real do indivíduo, uma vez que algumas condições iniciais podem não ser resolvidas corretamente. Entretanto, o tempo de execução diminui consideravelmente com o baixo número de simulações para cada indivíduo.

Ao longo das gerações, a média de aptidão da população cresceu, conforme o esperado. Na décima quinta geração, a maior parte dos indivíduos possuía a aptidão máxima, indicando que o pêndulo permaneceu equilibrado durante os 500 passos de simulação, em cada um dos 10 episódios com diferentes estados iniciais. Na última geração, muitos indivíduos apresentaram o operador de soma e a variável terminal que indicava o ângulo do pêndulo. Isto indica que esses elementos são membros importantes do conjunto primitivo, para a resolução do problema abordado. A tendência de aumento do comprimento dos indivíduos ao longo das gerações também pôde ser observada, indicando a importância de exercer o controle sobre esse processo. Os resultados obtidos através da programação genética foram similares aos alcançados com o algoritmo DQN.

Para o segundo problema abordado, ficou evidente a possível complexidade das soluções que surgem a partir do processo evolucionário, com alguns indivíduos apresentando mais de 70 nós. Por exemplo, o indivíduo da Figura \ref{fig:4ec-pendulumindiv1} apresenta 41 nós, representando uma função matemática capaz de equilibrar um sistema de alta complexidade. Isso demonstra a capacidade da programação genética em aproximar funções utilizando apenas o conceito da evolução biológica.

De certa forma, os indivíduos são sistemas que respondem às entradas, produzindo uma saída (ou sinal de controle), com isso, se assemelham muito à redes neurais, utilizadas de forma extensiva nos algoritmos DQN e DDPG. Umas das principais distinções entre as abordagens de aprendizagem por reforço e a programação genética reside na realização da otimização. Frequentemente, redes neurais são otimizadas a partir da propagação reversa do gradiente de uma função custo, em relação aos pesos da rede neural, enquanto métodos evolucionários são otimizações livres de gradiente (\textit{gradient free optimization}).

No ambiente do pêndulo duplo, o algoritmo DDPG apresentou-se superior. Entretanto, vale notar que não foram feitas mudanças significativas nos parâmetros da programação genética. É possível que algumas alterações, como, por exemplo, nas probabilidades de ocorrência dos operadores genéticos, tenham um efeito positivo no desempenho do algoritmo. Os hiperparâmetros do algoritmo DDPG não foram alterados, a partir da implementação utilizada \cite{stable-baselines}, com exceção do coeficiente de ruído das ações, um elemento que se mostrou importante para garantir o balanço entre exploração e convergência da rede neural.

Curiosamente, o melhor indivíduo da primeira execução da PG no pêndulo duplo possui uma estrutura bastante simples e, basicamente, verifica a velocidade do carrinho, utilizando-a para exercer um controle que varia entre os valores -1 e 1. Muitos dos problemas que podem ser resolvidos por buscas exaustivas no espaço de soluções costumam ser facilmente solucionados pela busca direcionada provida pela PG.

O problema do carro na ladeira evidenciou a dificuldade do algoritmo de aprendizagem por reforço, quando a função de recompensa não é muito informativa. Basicamente, o sinal de recompensa é estável, até que o objetivo final seja alcançado. Dessa forma, o aprendizado é demorado e a programação genética se mostrou superior em tempo de execução e desempenho.

Nos problemas da biblioteca Gym, a função de recompensa foi utilizada como medida de aptidão dos indivíduos. Devido à relação intrínseca dessa função com os processos de decisão de Markov, a recompensa deve ser, a cada instante de tempo, uma função de três argumentos: estado atual, ação e estado futuro. Isto impõe algumas restrições no direcionamento do comportamento do agente. Por exemplo, no problema do carro na ladeira, supondo que a busca inicial da PG, promovida pela inicialização da população, não seja capaz de produzir um indivíduo que atinja o objetivo, a evolução seria lenta ou não existente, já que todos os indivíduos apresentariam a menor aptidão possível. Contudo, a PG não impõe as mesmas restrições na avaliação do agente. Uma solução simples seria determinar como critério de aptidão a maior distância percorrida montanha acima, o que não poderia ser feito de forma simples na abordagem de aprendizagem por reforço. Pode-se concluir que na programação genética há um grau de liberdade maior na definição do critério de desempenho.

Em relação ao carro robô, foi possível notar que a programação genética multigênica pôde resolver o problema da trajetória de um veículo autônomo, que parte de um ponto fixo e alcança uma determinada posição e orientação. Os problemas 3 e 4 mostraram que é possível, muitas vezes, alcançar o objetivo partindo de posições e orientações iniciais aleatórias. Alguns testes foram realizados considerando uma variabilidade maior da \textit{pose} inicial, entretanto, não foram obtidos bons resultados. Em algumas situações iniciais foi possível perceber o movimento de \textit{zig-zag}, para alcançar o objetivo em um pequeno espaço físico. Vale notar que os algoritmos DQN e DDPG não são eficazes na resolução deste problema, e se mostra necessário a utilização de extensões ao processo de decisão de Markov e outros artifícios que buscam facilitar a exploração do espaço de observação. Além disso, a programação genética apresenta uma viabilidade maior de implementação, tendo em vista um veículo autônomo microcontrolado de pequeno porte. Enquanto os algoritmos de aprendizagem por reforço atuais geram políticas representadas por redes neurais, a programação genética produz expressões algébricas que são mais facilmente interpretadas \cite{politica}.

Em suma, este trabalho mostrou como integrar a formulação de problemas de aprendizagem por reforço com a aplicação da programação genética. Python é uma das principais linguagens de programação para o estudo e desenvolvimento de aplicações na área de aprendizagem de máquinas e as bibliotecas DEAP e Gym são referências em suas áreas, e buscam facilitar a implementação de novos algoritmos, assim como padronizar a verificação do desempenho de diferentes métodos.

Trabalhos futuros incluem a possibilidade de melhorar a implementação do ambiente criado para o carro robô, buscar novas implementações que integrem a programação genética com a aprendizagem por reforço e verificar o desempenho da PG em outros ambientes propostos pela biblioteca Gym.
